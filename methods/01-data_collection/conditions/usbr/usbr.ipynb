{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = Path(\"../../../../\")\n",
    "stations_metadata_path = Path(proj_dir, \"Data/insitu/metadata/stations.csv\")\n",
    "stations_attributes_path = Path(proj_dir, \"Data/insitu/metadata/dictionaries/stations_attributes.csv\")\n",
    "\n",
    "stations_attributes = pd.read_csv(stations_attributes_path)\n",
    "\n",
    "conditions_data = pd.read_csv(Path(proj_dir, \"Data/insitu/metadata/dictionaries/conditions_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conditions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "if not os.path.exists(stations_metadata_path):\n",
    "    stations_metadata = pd.DataFrame(columns=stations_attributes['Attribute_name'])\n",
    "    stations_metadata.to_csv(stations_metadata_path, index=False)\n",
    "\n",
    "stations_metadata = pd.read_csv(stations_metadata_path)\n",
    "\n",
    "usbr_stations_metadata = json.load(Path('stations.json').open(\"r\"))\n",
    "\n",
    "pcode_keys = usbr_stations_metadata[\"pcode_keys\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to format the url\n",
    "def format_url(station_name: str, pcodes: list, start: datetime, end: datetime):\n",
    "    \"\"\"Formats the url for the USBR PN data query.\n",
    "    Args:\n",
    "        station_name (str): The station name.\n",
    "        pcodes (list): The list of pcodes.\n",
    "        start (datetime): The start date.\n",
    "        end (datetime): The end date.\n",
    "    Returns:\n",
    "        url (str): The formatted url.\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        f\"https://www.usbr.gov/pn-bin/daily.pl?station={station_name.lower()}&format=csv&year={start.year}&month={start.month}&day={start.day}&year={end.year}&month={end.month}&day={end.day}\"\n",
    "        + \"\".join([\"&pcode=\" + pcode.strip(\" \").lower() for pcode in pcodes])\n",
    "    )\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to download the data for a station\n",
    "def download_data(station_name: str, pcodes: list, start: datetime, end: datetime, path: str):\n",
    "    \"\"\"Downloads the data for a station.\n",
    "    Args:\n",
    "        station_name (str): The station name.\n",
    "        pcodes (list): The list of pcodes.\n",
    "        start (datetime): The start date.\n",
    "        end (datetime): The end date.\n",
    "        path (str): The path to save the data.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # format the url\n",
    "    url = format_url(station_name, pcodes, start, end)\n",
    "\n",
    "    # download the data\n",
    "    # r = requests.get(url)\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.ConnectionError as e:\n",
    "        # sleep and try again\n",
    "        time.sleep(np.random.randint(20, 60))\n",
    "        r = requests.get(url)\n",
    "    # except requests.Timeout as e:\n",
    "    #     # stop the loop\n",
    "    #     break\n",
    "\n",
    "    # write the data to a csv file\n",
    "    with open(os.path.join(path, 'raw/usbr', station_name + \".csv\"), \"w\") as f:\n",
    "        f.write(r.text)\n",
    "\n",
    "    # read the csv file\n",
    "    with open(os.path.join(path, 'raw/usbr', station_name + \".csv\"), \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "\n",
    "    # remove the header\n",
    "    data = data[1:]\n",
    "\n",
    "    # define the column names\n",
    "    column_names = [\"date\"] + pcodes\n",
    "\n",
    "    # write the data to a csv file\n",
    "    with open(os.path.join(path, 'raw/usbr', station_name + \".csv\"), \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(column_names)\n",
    "        writer.writerows(data)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from deg min sec to decimal degrees\n",
    "def dms2dd(degrees, minutes=0, seconds=0, direction=None):\n",
    "    dd = float(degrees) + float(minutes) / 60 + float(seconds) / (60 * 60)\n",
    "    if direction == \"S\" or direction == \"W\":\n",
    "        dd *= -1\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process the downloaded data\n",
    "def postprocess_data(\n",
    "    station_name: str,\n",
    "    path: str,\n",
    "    grand_id: str = None,\n",
    "    pcodes: list = None,\n",
    "    pcode_keys: dict = None,\n",
    "):\n",
    "    if not grand_id:\n",
    "        grand_id = station_name\n",
    "\n",
    "    # read in the data\n",
    "    # print(path, \"raw/usbr\", \"{}.csv\".format(station_name.upper()))\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(path, \"raw/usbr\", \"{}.csv\".format(station_name.upper()))\n",
    "    )\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df[\"date\"] = df[\"date\"]\n",
    "\n",
    "    # convert the data to the correct units\n",
    "    for pcode in pcodes:\n",
    "        if pcode in pcode_keys.keys():\n",
    "            try:\n",
    "                pcode_keys[pcode][\"constant\"] = pcode_keys[pcode][\"constant\"]\n",
    "            except:\n",
    "                pcode_keys[pcode][\"constant\"] = None\n",
    "            \n",
    "            if pcode_keys[pcode][\"constant\"]:\n",
    "                new_df[pcode_keys[pcode][\"column_name\"]] = (\n",
    "                    df[pcode] * np.prod(pcode_keys[pcode][\"conversion_factors\"])\n",
    "                    + pcode_keys[pcode][\"constant\"]\n",
    "                )\n",
    "            else:\n",
    "                new_df[pcode_keys[pcode][\"column_name\"]] = df[pcode] * np.prod(\n",
    "                    pcode_keys[pcode][\"conversion_factors\"]\n",
    "                )\n",
    "\n",
    "    # save the data\n",
    "    new_df.to_csv(\n",
    "        # os.path.join(path, \"processed\", \"USBR_{}.csv\".format(grand_id)), index=False\n",
    "        os.path.join(path, \"processed\", \"USBR_{}.csv\".format(station_name)), index=False\n",
    "    )\n",
    "\n",
    "    return new_df.columns.tolist()\n",
    "    # print(\"processed data for {}\".format(station_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the station names\n",
    "# station_names = [\"crpo\", 'prv', 'prvo', 'kee', 'cle', 'crao']\n",
    "station_names = pd.read_csv(\"pcodes.csv\", header=None)[0]\n",
    "# grand_ids = [None, 91, '91_forebay', 55, 58, None]\n",
    "\n",
    "# define the start and end dates\n",
    "start_date = datetime.strptime(\"1982-01-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2024-02-28\", \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the stations json file\n",
    "with open(\"stations.json\", \"r\") as f:\n",
    "    stations_dict = json.load(f)\n",
    "\n",
    "# specify the download folder and make it the current working directory\n",
    "data_dir = proj_dir / \"Data/insitu/conditions\"\n",
    "\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path)\n",
    "os.makedirs(os.path.join(data_dir, \"raw/usbr\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, \"processed\"), exist_ok=True)\n",
    "\n",
    "# download the data for each station\n",
    "# for station_name, id in zip(station_names, grand_ids):\n",
    "for station_name in station_names:\n",
    "    # if pcodes exist for the station\n",
    "    if \"pcodes\" in stations_dict[station_name.upper()]:\n",
    "        # define the pcodes and pcode keys\n",
    "        pcodes = stations_dict[station_name.upper()][\"pcodes\"]\n",
    "        pcode_keys = stations_dict[\"pcode_keys\"]\n",
    "\n",
    "        # download the data\n",
    "        download_data(station_name.upper(), pcodes, start_date, end_date, data_dir)\n",
    "        # postprocess the data\n",
    "        station_conditions = postprocess_data(\n",
    "            station_name.upper(), data_dir, pcodes=pcodes, pcode_keys=pcode_keys\n",
    "        )\n",
    "\n",
    "        # print(conditions_data)\n",
    "\n",
    "        # update the metadata\n",
    "        station_ID = f\"USBR_{station_name}\"\n",
    "\n",
    "        # if station_ID not in stations_metadata[\"station_ID\"].values:\n",
    "        #     stations_metadata = pd.concat(\n",
    "        #         [\n",
    "        #             stations_metadata,\n",
    "        #             pd.DataFrame(\n",
    "        #                 {\n",
    "        #                     \"station_ID\": [station_ID],\n",
    "        #                     \"id_at_source\": [station_name.upper()],\n",
    "        #                     \"available_data\": [\"{}\"],\n",
    "        #                     \"source_URL\": [\n",
    "        #                         f\"https://www.usbr.gov/pn-bin/inventory.pl?site={station_name.upper()}&ui=true&interval=daily\"\n",
    "        #                     ],\n",
    "        #                     \"description\": [\n",
    "        #                         usbr_stations_metadata[station_name][\"description\"]\n",
    "        #                     ],\n",
    "        #                     \"latitude\": [\n",
    "        #                         dms2dd(\n",
    "        #                             *usbr_stations_metadata[station_name][\"latitude\"]\n",
    "        #                             .strip(\"-\")\n",
    "        #                             .split(\"-\"),\n",
    "        #                             direction=\"N\",\n",
    "        #                         )\n",
    "        #                     ],\n",
    "        #                     \"longitude\": [\n",
    "        #                         dms2dd(\n",
    "        #                             direction=\"W\",\n",
    "        #                             *usbr_stations_metadata[station_name][\"longitude\"]\n",
    "        #                             .strip(\"-\")\n",
    "        #                             .split(\"-\"),\n",
    "        #                         )\n",
    "        #                     ],\n",
    "        #                     \"site_params\": [\"{}\"],\n",
    "        #                 }\n",
    "        #             ),\n",
    "        #         ],\n",
    "        #         ignore_index=True,\n",
    "        #     )\n",
    "\n",
    "        # # update the available data\n",
    "        # availble_data = stations_metadata.loc[\n",
    "        #     stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "        # ].values[0]\n",
    "        # availble_data = json.loads(availble_data)\n",
    "\n",
    "        # add the parameters to the available data\n",
    "        # print(parameters[1:])\n",
    "        \n",
    "        for param in station_conditions:\n",
    "\n",
    "            if station_ID not in stations_metadata[\"station_ID\"].values:\n",
    "                new_stations_metadata = pd.DataFrame(\n",
    "                    {\n",
    "                        \"station_ID\": [station_ID],\n",
    "                        \"id_at_source\": [station_name.upper()],\n",
    "                        \"available_data\": [\"{}\"],\n",
    "                        \"source_URL\": ['{\"url\" : []}'],\n",
    "                        \"description\": [\n",
    "                            usbr_stations_metadata[station_name][\"description\"]\n",
    "                        ],\n",
    "                        \"latitude\": [\n",
    "                            dms2dd(\n",
    "                                *usbr_stations_metadata[station_name][\"latitude\"]\n",
    "                                .strip(\"-\")\n",
    "                                .split(\"-\"),\n",
    "                                direction=\"N\",\n",
    "                            )\n",
    "                        ],\n",
    "                        \"longitude\": [\n",
    "                            dms2dd(\n",
    "                                direction=\"W\",\n",
    "                                *usbr_stations_metadata[station_name][\"longitude\"]\n",
    "                                .strip(\"-\")\n",
    "                                .split(\"-\"),\n",
    "                            )\n",
    "                        ],\n",
    "                        \"site_params\": [\"{}\"],\n",
    "                    }\n",
    "                )\n",
    "                availble_data = json.loads(\n",
    "                    new_stations_metadata[\"available_data\"].values[0]\n",
    "                )\n",
    "                stations_metadata = pd.concat(\n",
    "                    [\n",
    "                        stations_metadata,\n",
    "                        new_stations_metadata,\n",
    "                    ],\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                availble_data = stations_metadata.loc[\n",
    "                    stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "                ].values[0]\n",
    "                availble_data = json.loads(availble_data)\n",
    "\n",
    "            # update source url\n",
    "            source_url = json.loads(\n",
    "                stations_metadata.loc[\n",
    "                    stations_metadata[\"station_ID\"] == station_ID, \"source_URL\"\n",
    "                ].values[0]\n",
    "            )\n",
    "\n",
    "            if (\n",
    "                f\"https://www.usbr.gov/pn-bin/inventory.pl?site={station_name.upper()}&ui=true&interval=daily\"\n",
    "                not in source_url[\"url\"]\n",
    "            ):\n",
    "                source_url[\"url\"].append(\n",
    "                    f\"https://www.usbr.gov/pn-bin/inventory.pl?site={station_name.upper()}&ui=true&interval=daily\"\n",
    "                )\n",
    "                stations_metadata.loc[\n",
    "                    stations_metadata[\"station_ID\"] == station_ID, \"source_URL\"\n",
    "                ] = json.dumps(source_url)\n",
    "\n",
    "            # check if there is \"conditions\"  in the available data\n",
    "            if \"conditions\" not in availble_data.values():\n",
    "                availble_data[\"conditions\"] = []\n",
    "\n",
    "            # # update the available data\n",
    "            # availble_data = stations_metadata.loc[\n",
    "            #     stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "            # ].values[0]\n",
    "            # availble_data = json.loads(availble_data)\n",
    "\n",
    "            # print((param not in availble_data[\"conditions\"]) and (param in conditions_data['Attribute_name'].to_list()))\n",
    "            if (param not in availble_data[\"conditions\"]) and (\n",
    "                param in conditions_data[\"Attribute_name\"].to_list()\n",
    "            ):\n",
    "\n",
    "                availble_data[\"conditions\"].append(param)\n",
    "\n",
    "                # if station_ID not in stations_metadata[\"station_ID\"].values:\n",
    "                #     new_stations_metadata[\"available_data\"] = json.dumps(availble_data)\n",
    "                #     stations_metadata = pd.concat(\n",
    "                #         [\n",
    "                #             stations_metadata,\n",
    "                #             new_stations_metadata,\n",
    "                #         ],\n",
    "                #         ignore_index=True,\n",
    "                #     )\n",
    "                # else:\n",
    "                #     # update the metadata\n",
    "                #     stations_metadata.loc[\n",
    "                #         stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "                #     ] = json.dumps(availble_data)\n",
    "\n",
    "                # update the metadata\n",
    "                stations_metadata.loc[\n",
    "                    stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "                ] = json.dumps(availble_data)\n",
    "\n",
    "                # # update the metadata\n",
    "                # stations_metadata.loc[\n",
    "                #     stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "                # ] = json.dumps(availble_data)\n",
    "\n",
    "                # save the metadata\n",
    "                stations_metadata.to_csv(stations_metadata_path, index=False)\n",
    "\n",
    "    print(\"processed data for {}\".format(station_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
