{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import geemap\n",
    "import ee\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from random import randint\n",
    "import json\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = Path(\"../../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = str(proj_dir / 'utils')\n",
    "sys.path.insert(0, utils)\n",
    "from sql import connect # utility functions for connecting to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_shp = Path(\n",
    "    proj_dir / \"Data/GIS/shapefiles/flowlines_to_reaches/bufferedReaches.shp\"\n",
    ")\n",
    "temperature_gauges_shp = Path(\n",
    "    proj_dir / \"Data/GIS/shapefiles/temperature_gauges.geojson\"\n",
    ")\n",
    "\n",
    "data_dir = Path(proj_dir, \"Data/LandsatTemperature\")\n",
    "# data_dir = Path(\"/Users/gdarkwah/eeDownloads\")\n",
    "os.makedirs(data_dir / \"reaches\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to MySQL database...\n",
      "Connection established.\n"
     ]
    }
   ],
   "source": [
    "# Create a connection object to the MySQL database\n",
    "# conn = connect.Connect(str(proj_dir / \"Methods/2.Data/DBManagement/mysql_config.ini\"))\n",
    "conn = connect.Connect(str(proj_dir / \".env/mysql_config.ini\"))\n",
    "connection = conn.conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(river_shp)\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "# save shapefile\n",
    "# gdf.to_file(data_dir/'rivers'/'rivers.shp')\n",
    "# gdf[gdf[\"GNIS_Name\"]==\"Columbia River\"].to_file(data_dir/'rivers'/'rivers.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map = geemap.Map()\n",
    "# Map\n",
    "# ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reservoirs = geemap.shp_to_ee(data_dir/'rivers'/'rivers.shp')\n",
    "# Map.addLayer(reservoirs, {}, \"Reservoirs\")\n",
    "# Map.centerObject(reservoirs, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the timeframe for the temperature data\n",
    "L9startDate = '2021-10-01'\n",
    "L8startDate = '2013-03-01'\n",
    "# L9endDate = '2023-08-31'\n",
    "# L8endDate = '2023-08-31'\n",
    "# L9startDate = \"2023-11-01\"\n",
    "# L8startDate = \"2023-11-01\"\n",
    "L9endDate = '2024-12-31'\n",
    "L8endDate = '2024-12-31'\n",
    "\n",
    "# startDate = L9startDate\n",
    "# endDate = L9endDate\n",
    "\n",
    "startDate = None # None for default or date in string format \"YYYY-MM-DD\"\n",
    "endDate = datetime.datetime.today()\n",
    "if startDate is None:\n",
    "    startDate = endDate - datetime.timedelta(days=90)\n",
    "else:\n",
    "    startDate = datetime.datetime.strptime(startDate, \"%Y-%m-%d\")\n",
    "\n",
    "# format dates as strings\n",
    "startDate = startDate.strftime(\"%Y-%m-%d\")\n",
    "endDate = endDate.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# ndwi threshold\n",
    "ndwi_threshold = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divideDates(startDate, endDate):\n",
    "    \"\"\"\n",
    "    Divide the timeframe into years\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    startDate: str\n",
    "        start date\n",
    "    endDate: str\n",
    "        end date\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        list of tuples of start and end dates\n",
    "    \"\"\"\n",
    "\n",
    "    # convert start and end dates to datetime objects\n",
    "    startDate_ = datetime.datetime.strptime(startDate, \"%Y-%m-%d\")\n",
    "    endDate_ = datetime.datetime.strptime(endDate, \"%Y-%m-%d\")\n",
    "\n",
    "    # get years from start and end dates\n",
    "    # startYear = pd.to_datetime(startDate).year\n",
    "    # endYear = pd.to_datetime(endDate).year\n",
    "    startYear = startDate_.year\n",
    "    endYear = endDate_.year\n",
    "\n",
    "    # divide the timeframe into years\n",
    "    dates = []\n",
    "    for year in range(startYear, endYear+1):\n",
    "        if year == startYear and year == endYear:\n",
    "            dates.append([startDate, endDate])\n",
    "        elif year == startYear:\n",
    "            dates.append([startDate, f\"{year}-12-31\"])\n",
    "        elif year == endYear:\n",
    "            # if the difference end date and start of the year is less than 30 days, then replace the end date of the previous append with the end date\n",
    "            # the purpose of this is to avoid having a date range of less than 30 days (especially at the beginning of the last year)\n",
    "            if (endDate_ - datetime.datetime(year, 1, 1)).days < 45:\n",
    "                dates[-1][1] = endDate\n",
    "            else:\n",
    "                dates.append([f\"{year}-01-01\", endDate])\n",
    "        else:\n",
    "            dates.append([f\"{year}-01-01\", f\"{year}-12-31\"])\n",
    "\n",
    "    return dates\n",
    "\n",
    "\n",
    "def prepL8(image):\n",
    "    \"\"\"\n",
    "    Prepare Landsat 8 image for analysis\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    image: ee.Image\n",
    "        Landsat 8 image\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ee.Image\n",
    "        prepared Landsat 8 image\n",
    "    \"\"\"\n",
    "\n",
    "    # develop masks for unwanted pixels (fill, dilated cloud, cirrus, cloud, cloud shadow, snow)\n",
    "    qa_mask = image.select(\"QA_PIXEL\").bitwiseAnd(int(\"111111\", 2)).eq(0)\n",
    "    saturation_mask = image.select(\"QA_RADSAT\").eq(0)\n",
    "\n",
    "    # apply scaling factors to the appropriate bands\n",
    "    def getFactorImage(factorNames):\n",
    "        factorList = image.toDictionary().select(factorNames).values()\n",
    "        return ee.Image.constant(factorList)\n",
    "\n",
    "    scaleImg = getFactorImage([\"REFLECTANCE_MULT_BAND_.|TEMPERATURE_MULT_BAND_ST_B10\"])\n",
    "    offsetImg = getFactorImage([\"REFLECTANCE_ADD_BAND_.|TEMPERATURE_ADD_BAND_ST_B10\"])\n",
    "    scaled = image.select(\"SR_B.|ST_B10\").multiply(scaleImg).add(offsetImg)\n",
    "\n",
    "    # replace original bands with scaled bands and apply masks\n",
    "    return (\n",
    "        image.addBands(scaled, overwrite=True)\n",
    "        .updateMask(qa_mask)\n",
    "        .updateMask(saturation_mask)\n",
    "    )\n",
    "\n",
    "\n",
    "def addNDWI(image):\n",
    "    \"\"\"\n",
    "    Add NDWI band to image\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    image: ee.Image\n",
    "        Landsat 8 image\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ee.Image\n",
    "        Landsat 8 image with NDWI band\n",
    "    \"\"\"\n",
    "\n",
    "    ndwi = image.expression(\n",
    "        \"NDWI = (green - NIR)/(green + NIR)\",\n",
    "        {\"green\": image.select(\"SR_B3\"), \"NIR\": image.select(\"SR_B5\")},\n",
    "    ).rename(\"NDWI\")\n",
    "\n",
    "    return image.addBands(ndwi)\n",
    "\n",
    "\n",
    "def addNDVI(image):\n",
    "    \"\"\"\n",
    "    Add NDVI band to image\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    image: ee.Image\n",
    "        Landsat 8 image\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ee.Image\n",
    "        Landsat 8 image with NDVI band\n",
    "    \"\"\"\n",
    "\n",
    "    # ndvi = image.expression(\n",
    "    #     \"NDVI = (NIR - red)/(NIR + red)\",\n",
    "    #     {\"red\": image.select(\"SR_B4\"), \"NIR\": image.select(\"SR_B5\")},\n",
    "    # ).rename(\"NDVI\")\n",
    "\n",
    "    ndvi = image.normalizedDifference([\"SR_B5\", \"SR_B4\"]).rename(\"NDVI\")\n",
    "\n",
    "    return image.addBands(ndvi)\n",
    "\n",
    "\n",
    "def addCelcius(image):\n",
    "    \"\"\"\n",
    "    Add Celcius band to image\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    image: ee.Image\n",
    "        Landsat 8 image\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ee.Image\n",
    "        Landsat 8 image with Celcius band\n",
    "    \"\"\"\n",
    "    celcius = image.select(\"ST_B10\").subtract(273.15).rename(\"Celcius\")\n",
    "\n",
    "    return image.addBands(celcius)\n",
    "\n",
    "\n",
    "def extractTempSeries(\n",
    "    reservoir,\n",
    "    startDate,\n",
    "    endDate,\n",
    "    ndwi_threshold=0.2,\n",
    "    imageCollection=\"LANDSAT/LC09/C02/T1_L2\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract temperature time series for a reservoir\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    reservoir: ee.Feature\n",
    "        reservoir\n",
    "    startDate: str\n",
    "        start date\n",
    "    endDate: str\n",
    "        end date\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ee.ImageCollection\n",
    "        temperature time series\n",
    "    \"\"\"\n",
    "\n",
    "    L8 = (\n",
    "        ee.ImageCollection(imageCollection)\n",
    "        .filterDate(startDate, endDate)\n",
    "        .filterBounds(reservoir)\n",
    "    )\n",
    "\n",
    "    # def extractWaterTemp(date):\n",
    "    def extractData(date):\n",
    "        date = ee.Date(date)\n",
    "        # prepare Landsat 8 image and add the NDWI band, and Celcius band\n",
    "        processedL8 = (\n",
    "            L8.filterDate(date, date.advance(1, \"day\"))\n",
    "            .map(prepL8)\n",
    "            .map(addCelcius)\n",
    "            .map(addNDWI)\n",
    "            .map(addNDVI)\n",
    "        )\n",
    "\n",
    "        # # get quality NDWI and use it as the water mask\n",
    "        # ndwi = processedL8.qualityMosaic(\"NDWI\").select(\"NDWI\")\n",
    "        # waterMask = ndwi.gte(ndwi_threshold)\n",
    "        # nonWaterMask = ndwi.lt(ndwi_threshold)\n",
    "\n",
    "        mosaic = processedL8.mosaic()\n",
    "        waterMask = mosaic.select(\"QA_PIXEL\").bitwiseAnd(int(\"10000000\", 2)).neq(0)\n",
    "        nonWaterMask = mosaic.select(\"QA_PIXEL\").bitwiseAnd(int(\"10000000\", 2)).eq(0)\n",
    "\n",
    "        # find the mean of the images in the collection\n",
    "        meanL8water = (\n",
    "            processedL8.reduce(ee.Reducer.mean())\n",
    "            # .addBands(ndwi, [\"NDWI\"], True)\n",
    "            .updateMask(waterMask)\n",
    "            .set(\"system:time_start\", date)\n",
    "        )\n",
    "        meanL8nonwater = (\n",
    "            processedL8.reduce(ee.Reducer.mean())\n",
    "            # .addBands(ndwi, [\"NDWI\"], True)\n",
    "            .updateMask(nonWaterMask)\n",
    "            .set(\"system:time_start\", date)\n",
    "        )\n",
    "\n",
    "        # get the mean temperature of the reservoir\n",
    "        watertemp = meanL8water.select([\"Celcius_mean\"]).reduceRegion(\n",
    "            reducer=ee.Reducer.mean(), geometry=reservoir.geometry(), scale=30\n",
    "        )\n",
    "        landtemp = meanL8nonwater.select([\"Celcius_mean\"]).reduceRegion(\n",
    "            reducer=ee.Reducer.mean(), geometry=reservoir.geometry(), scale=30\n",
    "        )\n",
    "        ndvi = meanL8nonwater.select([\"NDVI_mean\"]).reduceRegion(\n",
    "            reducer=ee.Reducer.mean(), geometry=reservoir.geometry(), scale=30\n",
    "        )\n",
    "\n",
    "        return ee.Feature(\n",
    "            None,\n",
    "            {\n",
    "                \"date\": date.format(\"YYYY-MM-dd\"),\n",
    "                \"watertemp(C)\": watertemp,\n",
    "                \"landtemp(C)\": landtemp,\n",
    "                \"NDVI\": ndvi,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def extractLandTemp(date):\n",
    "        date = ee.Date(date)\n",
    "        # prepare Landsat 8 image and add the NDWI band, and Celcius band\n",
    "        processedL8 = (\n",
    "            L8.filterDate(date, date.advance(1, \"day\"))\n",
    "            .map(prepL8)\n",
    "            .map(addCelcius)\n",
    "            .map(addNDWI)\n",
    "        )\n",
    "\n",
    "        # get quality NDWI and use it as the water mask\n",
    "        ndwi = processedL8.qualityMosaic(\"NDWI\").select(\"NDWI\")\n",
    "        nonWaterMask = ndwi.lt(ndwi_threshold)\n",
    "\n",
    "        # find the mean of the images in the collection\n",
    "        meanL8 = (\n",
    "            processedL8.reduce(ee.Reducer.mean())\n",
    "            .addBands(ndwi, [\"NDWI\"], True)\n",
    "            .updateMask(nonWaterMask)\n",
    "            .set(\"system:time_start\", date)\n",
    "        )\n",
    "\n",
    "        # get the mean temperature of the reservoir\n",
    "        temp = meanL8.select([\"Celcius_mean\"]).reduceRegion(\n",
    "            reducer=ee.Reducer.mean(), geometry=reservoir.geometry(), scale=30\n",
    "        )\n",
    "\n",
    "        return ee.Feature(None, {\"date\": date.format(\"YYYY-MM-dd\"), \"temp(C)\": temp})\n",
    "\n",
    "    dates = ee.List(\n",
    "        L8.map(\n",
    "            lambda image: ee.Feature(None, {\"date\": image.date().format(\"YYYY-MM-dd\")})\n",
    "        )\n",
    "        .distinct(\"date\")\n",
    "        .aggregate_array(\"date\")\n",
    "    )\n",
    "\n",
    "    # waterTempSeries = ee.FeatureCollection(dates.map(extractWaterTemp))\n",
    "    # landTempSeries = ee.FeatureCollection(dates.map(extractLandTemp))\n",
    "\n",
    "    dataSeries = ee.FeatureCollection(dates.map(extractData))\n",
    "\n",
    "    # return waterTempSeries, landTempSeries\n",
    "    return dataSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ee_to_df(featureCollection):\n",
    "    \"\"\"\n",
    "    Convert an ee.FeatureCollection to a pandas.DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    featureCollection: ee.FeatureCollection\n",
    "        feature collection\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    columns = featureCollection.first().propertyNames().getInfo()\n",
    "    rows = (\n",
    "        featureCollection.reduceColumns(ee.Reducer.toList(len(columns)), columns)\n",
    "        .values()\n",
    "        .get(0)\n",
    "        .getInfo()\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    df.drop(columns=[\"system:index\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def download_ee_csv(downloadUrl):\n",
    "    \"\"\"\n",
    "    Download an ee.FeatureCollection as a csv file\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    downloadUrl: str\n",
    "        download url\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(downloadUrl)\n",
    "    df.drop(columns=[\"system:index\", \".geo\"], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entryToDB(\n",
    "    data, table_name, reach_name, connection, date_col=\"date\", value_col=\"value\"\n",
    "):\n",
    "    data = data.copy()\n",
    "    data[date_col] = pd.to_datetime(data[date_col])\n",
    "    data = data[[date_col, value_col]]\n",
    "    data = data.dropna()\n",
    "    # data = data[data[value_col] != -9999]\n",
    "    data = data.sort_values(by=date_col)\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        query = f\"\"\"\n",
    "        INSERT INTO {table_name} (Date, ReachID, Value)\n",
    "        SELECT '{row[date_col]}', (SELECT ReachID FROM Reaches WHERE Name = \"{reach_name}\"), {row[value_col]}\n",
    "        WHERE NOT EXISTS (SELECT * FROM {table_name} WHERE Date = '{row[date_col]}' AND ReachID = (SELECT ReachID FROM Reaches WHERE Name = \"{reach_name}\"))\n",
    "        \"\"\"\n",
    "\n",
    "        cursor.execute(query)\n",
    "        connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reachwiseExtraction(\n",
    "    reaches,\n",
    "    reach_id,\n",
    "    startDate,\n",
    "    endDate,\n",
    "    ndwi_threshold=0.2,\n",
    "    imageCollection=\"LANDSAT/LC09/C02/T1_L2\",\n",
    "    checkpoint_path=None,\n",
    "    connection=None,\n",
    "):\n",
    "    if checkpoint_path is None:\n",
    "        checkpoint = {\"river_index\": 0, \"reach_index\": 0}\n",
    "    else:\n",
    "        with open(checkpoint_path, \"r\") as f:\n",
    "            checkpoint = json.load(f)\n",
    "\n",
    "    # if reach_ids is None:\n",
    "    #     ee_reach_ids = reaches.select(\"reach_id\", retainGeometry=False).getInfo()\n",
    "    #     reach_ids = [i[\"properties\"][\"reach_id\"] for i in ee_reach_ids[\"features\"]][\n",
    "    #         checkpoint[\"reach_index\"] :\n",
    "    #     ]\n",
    "    #     # reach_ids = gdf[\"reach_id\"].tolist()\n",
    "\n",
    "    # extract temperature time series for each reservoir\n",
    "    # for reach_id in reach_ids:\n",
    "    # print(f\"Reach {reach_id} started!\")\n",
    "    dates = divideDates(startDate, endDate)\n",
    "    waterTempSeriesList = []\n",
    "    landTempSeriesList = []\n",
    "\n",
    "    dataSeriesList = []\n",
    "\n",
    "    # if os.path.isfile(data_dir / \"reaches\" / f\"{reach_id}_watertemp.csv\"):\n",
    "    #     existing_df = pd.read_csv(\n",
    "    #         data_dir / \"reaches\" / f\"{reach_id}_watertemp.csv\"\n",
    "    #     )\n",
    "    #     existing_df[\"date\"] = pd.to_datetime(existing_df[\"date\"])\n",
    "    #     waterTempSeriesList.append(existing_df)\n",
    "    #     # print(\"File exists!\")\n",
    "\n",
    "    # if os.path.isfile(data_dir / \"reaches\" / f\"{reach_id}_landtemp.csv\"):\n",
    "    #     existing_df = pd.read_csv(data_dir / \"reaches\" / f\"{reach_id}_landtemp.csv\")\n",
    "    #     existing_df[\"date\"] = pd.to_datetime(existing_df[\"date\"])\n",
    "    #     landTempSeriesList.append(existing_df)\n",
    "    #     # print(\"File exists!\")\n",
    "\n",
    "    # if os.path.isfile(data_dir / \"reaches\" / f\"{reach_id}.csv\"):\n",
    "    #     existing_df = pd.read_csv(data_dir / \"reaches\" / f\"{reach_id}.csv\")\n",
    "    #     existing_df[\"date\"] = pd.to_datetime(existing_df[\"date\"])\n",
    "    #     dataSeriesList.append(existing_df)\n",
    "    #     # print(\"File exists!\")\n",
    "\n",
    "    # if os.path.isfile(data_dir / \"reaches\" / f\"{reach_id}.csv\"):\n",
    "    #     existing_df = pd.read_csv(data_dir / \"reaches\" / f\"{reach_id}.csv\")\n",
    "    #     existing_df[\"date\"] = pd.to_datetime(existing_df[\"date\"])\n",
    "    #     dataSeriesList.append(existing_df)\n",
    "\n",
    "    for date in dates:\n",
    "        startDate_ = date[0]\n",
    "        endDate_ = date[1]\n",
    "\n",
    "        reservoir = reaches.filter(ee.Filter.eq(\"reach_id\", ee.String(reach_id)))\n",
    "        # waterTempSeries, landTempSeries= extractTempSeries(\n",
    "        #     reservoir, startDate_, endDate_, ndwi_threshold, imageCollection\n",
    "        # )\n",
    "        # waterTempSeries = geemap.ee_to_pandas(waterTempSeries)\n",
    "        # landTempSeries = geemap.ee_to_pandas(landTempSeries)\n",
    "        dataSeries = extractTempSeries(\n",
    "            reservoir, startDate_, endDate_, ndwi_threshold, imageCollection\n",
    "        )\n",
    "        dataSeries = geemap.ee_to_pandas(dataSeries)\n",
    "\n",
    "        # convert date column to datetime\n",
    "        # waterTempSeries[\"date\"] = pd.to_datetime(waterTempSeries[\"date\"])\n",
    "        # landTempSeries[\"date\"] = pd.to_datetime(landTempSeries[\"date\"])\n",
    "        dataSeries[\"date\"] = pd.to_datetime(dataSeries[\"date\"])\n",
    "\n",
    "        # waterTempSeries[\"temp(C)\"] = (\n",
    "        #     waterTempSeries[\"temp(C)\"]\n",
    "        #     .apply(lambda x: x[\"Celcius_mean\"])\n",
    "        #     .astype(float)\n",
    "        # )\n",
    "        # landTempSeries[\"temp(C)\"] = (\n",
    "        #     landTempSeries[\"temp(C)\"]\n",
    "        #     .apply(lambda x: x[\"Celcius_mean\"])\n",
    "        #     .astype(float)\n",
    "        # )\n",
    "\n",
    "        dataSeries[\"watertemp(C)\"] = (\n",
    "            dataSeries[\"watertemp(C)\"]\n",
    "            .apply(lambda x: x[\"Celcius_mean\"])\n",
    "            .astype(float)\n",
    "        )\n",
    "        dataSeries[\"landtemp(C)\"] = (\n",
    "            dataSeries[\"landtemp(C)\"]\n",
    "            .apply(lambda x: x[\"Celcius_mean\"])\n",
    "            .astype(float)\n",
    "        )\n",
    "        dataSeries[\"NDVI\"] = (\n",
    "            dataSeries[\"NDVI\"].apply(lambda x: x[\"NDVI_mean\"]).astype(float)\n",
    "        )\n",
    "\n",
    "        # append time series to list\n",
    "        # waterTempSeriesList.append(waterTempSeries)\n",
    "        # landTempSeriesList.append(landTempSeries)\n",
    "        dataSeriesList.append(dataSeries)\n",
    "\n",
    "        s_time = randint(5, 10)\n",
    "        time.sleep(s_time)\n",
    "\n",
    "    # concatenate all time series\n",
    "    # waterTempSeries_df = pd.concat(waterTempSeriesList, ignore_index=True)\n",
    "    # landTempSeries_df = pd.concat(landTempSeriesList, ignore_index=True)\n",
    "    dataSeries_df = pd.concat(dataSeriesList, ignore_index=True)\n",
    "\n",
    "    # sort by date\n",
    "    # waterTempSeries_df.sort_values(by=\"date\", inplace=True)\n",
    "    # landTempSeries_df.sort_values(by=\"date\", inplace=True)\n",
    "    dataSeries_df.sort_values(by=\"date\", inplace=True)\n",
    "    # #drop null values\n",
    "    # # waterTempSeries_df.dropna(inplace=True)\n",
    "    # # landTempSeries_df.dropna(inplace=True)\n",
    "    # dataSeries_df.dropna(inplace=True)\n",
    "    # remove duplicates\n",
    "    # waterTempSeries_df.drop_duplicates(subset=\"date\", inplace=True)\n",
    "    # landTempSeries_df.drop_duplicates(subset=\"date\", inplace=True)\n",
    "    dataSeries_df.drop_duplicates(subset=\"date\", inplace=True)\n",
    "\n",
    "    # save time series to csv\n",
    "    # waterTempSeries_df.to_csv(\n",
    "    #     data_dir / \"reaches\" / f\"{reach_id}_watertemp.csv\", index=False\n",
    "    # )\n",
    "    # landTempSeries_df.to_csv(\n",
    "    #     data_dir / \"reaches\" / f\"{reach_id}_landtemp.csv\", index=False\n",
    "    # )\n",
    "    # dataSeries_df.to_csv(\n",
    "    #     data_dir / \"reaches\" / f\"{reach_id}.csv\", index=False\n",
    "    # )\n",
    "\n",
    "    # land temp\n",
    "    entryToDB(\n",
    "        dataSeries_df,\n",
    "        \"ReachLandsatLandTemp\",\n",
    "        reach_id,\n",
    "        connection,\n",
    "        date_col=\"date\",\n",
    "        value_col=\"landtemp(C)\",\n",
    "    )\n",
    "    # water temp\n",
    "    entryToDB(\n",
    "        dataSeries_df,\n",
    "        \"ReachLandsatWaterTemp\",\n",
    "        reach_id,\n",
    "        connection,\n",
    "        date_col=\"date\",\n",
    "        value_col=\"watertemp(C)\",\n",
    "    )\n",
    "    # NDVI\n",
    "    entryToDB(\n",
    "        dataSeries_df,\n",
    "        \"ReachNDVI\",\n",
    "        reach_id,\n",
    "        connection,\n",
    "        date_col=\"date\",\n",
    "        value_col=\"NDVI\",\n",
    "    )\n",
    "\n",
    "    # checkpoint[\"reach_index\"] += 1\n",
    "    # json.dump(checkpoint, open(checkpoint_path, \"w\"))\n",
    "    # print(f\"Reach {reach_id} done!\")\n",
    "    # s_time = randint(30, 60)\n",
    "    # time.sleep(s_time)\n",
    "\n",
    "    # print(\"All done!\")\n",
    "\n",
    "    # TODO: Delete this section onwards\n",
    "    # # extract temperature time series for each reservoir\n",
    "    # for reach_id in reach_ids:\n",
    "    #     # print(f\"Reach {reach_id} started!\")\n",
    "    #     dates = divideDates(startDate, endDate)\n",
    "    #     waterTempSeriesList = []\n",
    "    #     landTempSeriesList = []\n",
    "\n",
    "    #     dataSeriesList = []\n",
    "\n",
    "    #     # if os.path.isfile(data_dir / \"reaches\" / f\"{reach_id}_watertemp.csv\"):\n",
    "    #     #     existing_df = pd.read_csv(\n",
    "    #     #         data_dir / \"reaches\" / f\"{reach_id}_watertemp.csv\"\n",
    "    #     #     )\n",
    "    #     #     existing_df[\"date\"] = pd.to_datetime(existing_df[\"date\"])\n",
    "    #     #     waterTempSeriesList.append(existing_df)\n",
    "    #     #     # print(\"File exists!\")\n",
    "\n",
    "    #     # if os.path.isfile(data_dir / \"reaches\" / f\"{reach_id}_landtemp.csv\"):\n",
    "    #     #     existing_df = pd.read_csv(data_dir / \"reaches\" / f\"{reach_id}_landtemp.csv\")\n",
    "    #     #     existing_df[\"date\"] = pd.to_datetime(existing_df[\"date\"])\n",
    "    #     #     landTempSeriesList.append(existing_df)\n",
    "    #     #     # print(\"File exists!\")\n",
    "\n",
    "    #     # if os.path.isfile(data_dir / \"reaches\" / f\"{reach_id}.csv\"):\n",
    "    #     #     existing_df = pd.read_csv(data_dir / \"reaches\" / f\"{reach_id}.csv\")\n",
    "    #     #     existing_df[\"date\"] = pd.to_datetime(existing_df[\"date\"])\n",
    "    #     #     dataSeriesList.append(existing_df)\n",
    "    #     #     # print(\"File exists!\")\n",
    "\n",
    "    #     for date in dates:\n",
    "    #         startDate_ = date[0]\n",
    "    #         endDate_ = date[1]\n",
    "\n",
    "    #         reservoir = reaches.filter(ee.Filter.eq(\"reach_id\", ee.String(reach_id)))\n",
    "    #         # waterTempSeries, landTempSeries= extractTempSeries(\n",
    "    #         #     reservoir, startDate_, endDate_, ndwi_threshold, imageCollection\n",
    "    #         # )\n",
    "    #         # waterTempSeries = geemap.ee_to_pandas(waterTempSeries)\n",
    "    #         # landTempSeries = geemap.ee_to_pandas(landTempSeries)\n",
    "    #         dataSeries = extractTempSeries(\n",
    "    #             reservoir, startDate_, endDate_, ndwi_threshold, imageCollection\n",
    "    #         )\n",
    "    #         dataSeries = geemap.ee_to_pandas(dataSeries)\n",
    "\n",
    "    #         # convert date column to datetime\n",
    "    #         # waterTempSeries[\"date\"] = pd.to_datetime(waterTempSeries[\"date\"])\n",
    "    #         # landTempSeries[\"date\"] = pd.to_datetime(landTempSeries[\"date\"])\n",
    "    #         dataSeries[\"date\"] = pd.to_datetime(dataSeries[\"date\"])\n",
    "\n",
    "    #         # waterTempSeries[\"temp(C)\"] = (\n",
    "    #         #     waterTempSeries[\"temp(C)\"]\n",
    "    #         #     .apply(lambda x: x[\"Celcius_mean\"])\n",
    "    #         #     .astype(float)\n",
    "    #         # )\n",
    "    #         # landTempSeries[\"temp(C)\"] = (\n",
    "    #         #     landTempSeries[\"temp(C)\"]\n",
    "    #         #     .apply(lambda x: x[\"Celcius_mean\"])\n",
    "    #         #     .astype(float)\n",
    "    #         # )\n",
    "\n",
    "    #         dataSeries[\"watertemp(C)\"] = (\n",
    "    #             dataSeries[\"watertemp(C)\"]\n",
    "    #             .apply(lambda x: x[\"Celcius_mean\"])\n",
    "    #             .astype(float)\n",
    "    #         )\n",
    "    #         dataSeries[\"landtemp(C)\"] = (\n",
    "    #             dataSeries[\"landtemp(C)\"]\n",
    "    #             .apply(lambda x: x[\"Celcius_mean\"])\n",
    "    #             .astype(float)\n",
    "    #         )\n",
    "    #         dataSeries[\"NDVI\"] = (\n",
    "    #             dataSeries[\"NDVI\"].apply(lambda x: x[\"NDVI_mean\"]).astype(float)\n",
    "    #         )\n",
    "\n",
    "    #         # append time series to list\n",
    "    #         # waterTempSeriesList.append(waterTempSeries)\n",
    "    #         # landTempSeriesList.append(landTempSeries)\n",
    "    #         dataSeriesList.append(dataSeries)\n",
    "\n",
    "    #         s_time = randint(5, 10)\n",
    "    #         time.sleep(s_time)\n",
    "\n",
    "    #     # concatenate all time series\n",
    "    #     # waterTempSeries_df = pd.concat(waterTempSeriesList, ignore_index=True)\n",
    "    #     # landTempSeries_df = pd.concat(landTempSeriesList, ignore_index=True)\n",
    "    #     dataSeries_df = pd.concat(dataSeriesList, ignore_index=True)\n",
    "\n",
    "    #     # sort by date\n",
    "    #     # waterTempSeries_df.sort_values(by=\"date\", inplace=True)\n",
    "    #     # landTempSeries_df.sort_values(by=\"date\", inplace=True)\n",
    "    #     dataSeries_df.sort_values(by=\"date\", inplace=True)\n",
    "    #     # #drop null values\n",
    "    #     # # waterTempSeries_df.dropna(inplace=True)\n",
    "    #     # # landTempSeries_df.dropna(inplace=True)\n",
    "    #     # dataSeries_df.dropna(inplace=True)\n",
    "    #     # remove duplicates\n",
    "    #     # waterTempSeries_df.drop_duplicates(subset=\"date\", inplace=True)\n",
    "    #     # landTempSeries_df.drop_duplicates(subset=\"date\", inplace=True)\n",
    "    #     dataSeries_df.drop_duplicates(subset=\"date\", inplace=True)\n",
    "\n",
    "    #     # save time series to csv\n",
    "    #     # waterTempSeries_df.to_csv(\n",
    "    #     #     data_dir / \"reaches\" / f\"{reach_id}_watertemp.csv\", index=False\n",
    "    #     # )\n",
    "    #     # landTempSeries_df.to_csv(\n",
    "    #     #     data_dir / \"reaches\" / f\"{reach_id}_landtemp.csv\", index=False\n",
    "    #     # )\n",
    "    #     # dataSeries_df.to_csv(\n",
    "    #     #     data_dir / \"reaches\" / f\"{reach_id}.csv\", index=False\n",
    "    #     # )\n",
    "\n",
    "    #     # land temp\n",
    "    #     entryToDB(\n",
    "    #         dataSeries_df,\n",
    "    #         \"ReachLandsatLandTemp\",\n",
    "    #         reach_id,\n",
    "    #         connection,\n",
    "    #         date_col=\"date\",\n",
    "    #         value_col=\"landtemp(C)\",\n",
    "    #     )\n",
    "    #     # water temp\n",
    "    #     entryToDB(\n",
    "    #         dataSeries_df,\n",
    "    #         \"ReachLandsatWaterTemp\",\n",
    "    #         reach_id,\n",
    "    #         connection,\n",
    "    #         date_col=\"date\",\n",
    "    #         value_col=\"watertemp(C)\",\n",
    "    #     )\n",
    "    #     # NDVI\n",
    "    #     entryToDB(\n",
    "    #         dataSeries_df,\n",
    "    #         \"ReachNDVI\",\n",
    "    #         reach_id,\n",
    "    #         connection,\n",
    "    #         date_col=\"date\",\n",
    "    #         value_col=\"NDVI\",\n",
    "    #     )\n",
    "\n",
    "    #     checkpoint[\"reach_index\"] += 1\n",
    "    #     json.dump(checkpoint, open(checkpoint_path, \"w\"))\n",
    "    #     print(f\"Reach {reach_id} done!\")\n",
    "    #     # s_time = randint(30, 60)\n",
    "    #     # time.sleep(s_time)\n",
    "\n",
    "    # # print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(river_shp)\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "rivers = gdf[\"GNIS_Name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runExtraction(data_dir, checkpoint_path=None, connection=None):\n",
    "    if checkpoint_path is None:\n",
    "        checkpoint = {\"river_index\": 0, \"reach_index\": 0}\n",
    "    else:\n",
    "        with open(checkpoint_path, \"r\") as f:\n",
    "            checkpoint = json.load(f)\n",
    "\n",
    "    # gdf = gpd.read_file(river_shp)\n",
    "    # gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "    # unique_rivers = gdf[\"GNIS_Name\"].unique()\n",
    "    # unique_rivers = gdf[\"GNIS_Name\"].unique()[checkpoint[\"river_index\"]:]\n",
    "    unique_rivers = rivers[checkpoint[\"river_index\"] :]\n",
    "    # unique_rivers = redo_rivers[checkpoint[\"river_index\"]:]\n",
    "\n",
    "    for river in unique_rivers:\n",
    "        gdf[gdf[\"GNIS_Name\"] == river].to_file(data_dir / \"reaches\" / \"rivers.shp\")\n",
    "        reach_ids = gdf[gdf[\"GNIS_Name\"] == river][\"reach_id\"].tolist()\n",
    "        reach_ids = reach_ids[checkpoint[\"reach_index\"] :]\n",
    "\n",
    "        reaches = geemap.shp_to_ee(data_dir / \"reaches\" / \"rivers.shp\")\n",
    "\n",
    "        if reach_ids is None:\n",
    "            ee_reach_ids = reaches.select(\"reach_id\", retainGeometry=False).getInfo()\n",
    "            reach_ids = [i[\"properties\"][\"reach_id\"] for i in ee_reach_ids[\"features\"]][\n",
    "                checkpoint[\"reach_index\"] :\n",
    "            ]\n",
    "            # reach_ids = gdf[\"reach_id\"].tolist()\n",
    "\n",
    "        for reach_id in reach_ids:\n",
    "            # Landsat8 Data\n",
    "            reachwiseExtraction(\n",
    "                reaches,\n",
    "                reach_id,\n",
    "                L8startDate,\n",
    "                L8endDate,\n",
    "                ndwi_threshold,\n",
    "                imageCollection=\"LANDSAT/LC08/C02/T1_L2\",\n",
    "                checkpoint_path=checkpoint_path,\n",
    "                connection=connection,\n",
    "            )\n",
    "\n",
    "            # Landsat9 Data\n",
    "            reachwiseExtraction(\n",
    "                reaches,\n",
    "                reach_id,\n",
    "                L9startDate,\n",
    "                L9endDate,\n",
    "                ndwi_threshold,\n",
    "                imageCollection=\"LANDSAT/LC09/C02/T1_L2\",\n",
    "                checkpoint_path=checkpoint_path,\n",
    "                connection=connection,\n",
    "            )\n",
    "\n",
    "            checkpoint[\"reach_index\"] += 1\n",
    "            json.dump(checkpoint, open(checkpoint_path, \"w\"))\n",
    "            print(f\"Reach {reach_id} done!\")\n",
    "\n",
    "        checkpoint[\"reach_index\"] = 0\n",
    "        checkpoint[\"river_index\"] += 1\n",
    "        json.dump(checkpoint, open(checkpoint_path, \"w\"))\n",
    "\n",
    "        # s_time = randint(30,120)\n",
    "        # time.sleep(s_time)\n",
    "\n",
    "        print(f\"{river} done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf[\"GNIS_Name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reach Snake_River_98 done!\n",
      "Reach Snake_River_99 done!\n",
      "Reach Snake_River_100 done!\n",
      "Reach Snake_River_101 done!\n",
      "Reach Snake_River_102 done!\n",
      "Reach Snake_River_103 done!\n",
      "Reach Snake_River_104 done!\n",
      "Reach Snake_River_105 done!\n",
      "Reach Snake_River_106 done!\n",
      "Reach Snake_River_107 done!\n",
      "Reach Snake_River_108 done!\n",
      "Reach Snake_River_109 done!\n",
      "Reach Snake_River_110 done!\n",
      "Reach Snake_River_111 done!\n",
      "Reach Snake_River_112 done!\n",
      "Reach Snake_River_113 done!\n",
      "Reach Snake_River_114 done!\n",
      "Reach Snake_River_115 done!\n",
      "Reach Snake_River_116 done!\n",
      "Reach Snake_River_117 done!\n",
      "Reach Snake_River_118 done!\n",
      "Reach Snake_River_119 done!\n",
      "Reach Snake_River_120 done!\n",
      "Reach Snake_River_121 done!\n",
      "Reach Snake_River_122 done!\n",
      "Reach Snake_River_123 done!\n",
      "Reach Snake_River_124 done!\n",
      "Reach Snake_River_125 done!\n",
      "Reach Snake_River_126 done!\n",
      "Reach Snake_River_127 done!\n",
      "Reach Snake_River_128 done!\n",
      "Error: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Sleeping for 114 seconds...\n",
      "Restarting from checkpoint...\n",
      "Reach Snake_River_129 done!\n",
      "Reach Snake_River_130 done!\n",
      "Reach Snake_River_131 done!\n",
      "Reach Snake_River_132 done!\n",
      "Reach Snake_River_133 done!\n",
      "Reach Snake_River_134 done!\n",
      "Reach Snake_River_135 done!\n",
      "Reach Snake_River_136 done!\n",
      "Reach Snake_River_137 done!\n",
      "Reach Snake_River_138 done!\n",
      "Reach Snake_River_139 done!\n",
      "Reach Snake_River_140 done!\n",
      "Reach Snake_River_141 done!\n",
      "Error: ('Connection aborted.', OSError(22, 'Invalid argument'))\n",
      "Sleeping for 95 seconds...\n",
      "Restarting from checkpoint...\n",
      "Reach Snake_River_142 done!\n",
      "Reach Snake_River_143 done!\n",
      "Reach Snake_River_144 done!\n",
      "Reach Snake_River_145 done!\n",
      "Reach Snake_River_146 done!\n",
      "Reach Snake_River_147 done!\n",
      "Reach Snake_River_148 done!\n",
      "Reach Snake_River_149 done!\n",
      "Reach Snake_River_150 done!\n",
      "Reach Snake_River_151 done!\n",
      "Reach Snake_River_152 done!\n",
      "Reach Snake_River_153 done!\n",
      "Reach Snake_River_154 done!\n",
      "Reach Snake_River_155 done!\n",
      "Reach Snake_River_156 done!\n",
      "Reach Snake_River_157 done!\n",
      "Reach Snake_River_158 done!\n",
      "Reach Snake_River_159 done!\n",
      "Reach Snake_River_160 done!\n",
      "Reach Snake_River_161 done!\n",
      "Reach Snake_River_162 done!\n",
      "Reach Snake_River_163 done!\n",
      "Reach Snake_River_164 done!\n",
      "Reach Snake_River_165 done!\n",
      "Reach Snake_River_166 done!\n",
      "Reach Snake_River_167 done!\n",
      "Reach Snake_River_168 done!\n",
      "Reach Snake_River_169 done!\n",
      "Reach Snake_River_170 done!\n",
      "Reach Snake_River_171 done!\n",
      "Reach Snake_River_172 done!\n",
      "Reach Snake_River_173 done!\n",
      "Reach Snake_River_174 done!\n",
      "Error: ('Connection aborted.', OSError(22, 'Invalid argument'))\n",
      "Sleeping for 65 seconds...\n",
      "Restarting from checkpoint...\n",
      "Error: ('Connection aborted.', OSError(22, 'Invalid argument'))\n",
      "Sleeping for 111 seconds...\n",
      "Restarting from checkpoint...\n",
      "Reach Snake_River_176 done!\n",
      "Reach Snake_River_177 done!\n",
      "Reach Snake_River_178 done!\n",
      "Reach Snake_River_179 done!\n",
      "Reach Snake_River_180 done!\n",
      "Reach Snake_River_181 done!\n",
      "Reach Snake_River_182 done!\n",
      "Reach Snake_River_183 done!\n",
      "Reach Snake_River_184 done!\n",
      "Reach Snake_River_185 done!\n",
      "Reach Snake_River_186 done!\n",
      "Reach Snake_River_187 done!\n",
      "Reach Snake_River_188 done!\n",
      "Reach Snake_River_189 done!\n",
      "Reach Snake_River_190 done!\n",
      "Reach Snake_River_191 done!\n",
      "Reach Snake_River_192 done!\n",
      "Reach Snake_River_193 done!\n",
      "Reach Snake_River_194 done!\n",
      "Error: HTTPSConnectionPool(host='earthengine.googleapis.com', port=443): Max retries exceeded with url: /v1alpha/projects/earthengine-legacy/value:compute?prettyPrint=false&alt=json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x153c02010>: Failed to resolve 'earthengine.googleapis.com' ([Errno 8] nodename nor servname provided, or not known)\"))\n",
      "Sleeping for 64 seconds...\n",
      "Restarting from checkpoint...\n",
      "Reach Snake_River_195 done!\n",
      "Reach Snake_River_196 done!\n",
      "Reach Snake_River_197 done!\n",
      "Reach Snake_River_198 done!\n",
      "Reach Snake_River_199 done!\n",
      "Error: ('Connection aborted.', OSError(22, 'Invalid argument'))\n",
      "Sleeping for 88 seconds...\n",
      "Restarting from checkpoint...\n",
      "Reach Snake_River_200 done!\n",
      "Reach Snake_River_201 done!\n",
      "Reach Snake_River_202 done!\n",
      "Reach Snake_River_203 done!\n",
      "Reach Snake_River_204 done!\n",
      "Reach Snake_River_205 done!\n",
      "Reach Snake_River_206 done!\n",
      "Reach Snake_River_207 done!\n",
      "Error: ('Connection aborted.', OSError(22, 'Invalid argument'))\n",
      "Sleeping for 57 seconds...\n",
      "Restarting from checkpoint...\n",
      "Reach Snake_River_208 done!\n",
      "Reach Snake_River_209 done!\n",
      "Error: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))\n",
      "Sleeping for 92 seconds...\n",
      "Restarting from checkpoint...\n",
      "Reach Snake_River_209 done!\n",
      "Reach Snake_River_210 done!\n",
      "Error: ('Connection aborted.', OSError(22, 'Invalid argument'))\n",
      "Sleeping for 81 seconds...\n",
      "Restarting from checkpoint...\n",
      "Reach Snake_River_211 done!\n",
      "Reach Snake_River_212 done!\n",
      "Error: ('Connection aborted.', OSError(22, 'Invalid argument'))\n",
      "Sleeping for 76 seconds...\n",
      "Restarting from checkpoint...\n",
      "Reach Snake_River_213 done!\n",
      "Reach Snake_River_214 done!\n",
      "Error: ('Connection aborted.', OSError(22, 'Invalid argument'))\n",
      "Sleeping for 105 seconds...\n",
      "Restarting from checkpoint...\n",
      "Reach Snake_River_215 done!\n",
      "Reach Snake_River_216 done!\n",
      "Reach Snake_River_217 done!\n",
      "Error: ('Connection aborted.', OSError(22, 'Invalid argument'))\n",
      "Sleeping for 77 seconds...\n",
      "Restarting from checkpoint...\n",
      "Reach Snake_River_216 done!\n",
      "Reach Snake_River_217 done!\n",
      "Reach Snake_River_218 done!\n",
      "Reach Snake_River_219 done!\n",
      "Snake River done!\n",
      "Reach Kootenay_River_1 done!\n",
      "Reach Kootenay_River_2 done!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(data_dir / \"reaches\" / \"checkpoint.json\", \"r\") as f:\n",
    "        checkpoint = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Creating new checkpoint...\")\n",
    "    checkpoint = {\"river_index\": 0, \"reach_index\": 0}\n",
    "    # save checkpoint\n",
    "    json.dump(checkpoint, open(data_dir / \"reaches\" / \"checkpoint.json\", \"w\"))\n",
    "\n",
    "repeated_tries = 0\n",
    "\n",
    "# while checkpoint[\"river_index\"] < len(gdf[\"GNIS_Name\"].unique()):\n",
    "while checkpoint[\"river_index\"] < len(rivers):\n",
    "    # while checkpoint[\"river_index\"] < len(redo_rivers):\n",
    "    try:\n",
    "        runExtraction(data_dir, data_dir / \"reaches\" / \"checkpoint.json\", connection)\n",
    "        repeated_tries = 0  # reset repeated_tries\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        # sleep for 0.5 - 3 minutes\n",
    "        s_time = randint(30, 120)\n",
    "        print(f\"Sleeping for {s_time} seconds...\")\n",
    "        time.sleep(s_time)\n",
    "        print(\"Restarting from checkpoint...\")  # restart from checkpoint\n",
    "\n",
    "        repeated_tries += 1  # increment repeated_tries\n",
    "\n",
    "        # if repeated_tries > 3, increment river_index and reset reach_index\n",
    "        if repeated_tries > 3:\n",
    "            checkpoint[\"reach_index\"] += 1\n",
    "            current_river = gdf[\"GNIS_Name\"].unique()[checkpoint[\"river_index\"]]\n",
    "            if checkpoint[\"reach_index\"] >= len(\n",
    "                gdf[gdf[\"GNIS_Name\"] == current_river][\"reach_id\"].tolist()\n",
    "            ):\n",
    "                checkpoint[\"reach_index\"] = 0\n",
    "                checkpoint[\"river_index\"] += 1\n",
    "            repeated_tries = 0\n",
    "\n",
    "            # save checkpoint\n",
    "            json.dump(checkpoint, open(data_dir / \"reaches\" / \"checkpoint.json\", \"w\"))\n",
    "    finally:\n",
    "        # save checkpoint\n",
    "        with open(data_dir / \"reaches\" / \"checkpoint.json\", \"r\") as f:\n",
    "            checkpoint = json.load(f)\n",
    "\n",
    "# reset checkpoint if all rivers are done\n",
    "# if checkpoint[\"river_index\"] >= len(gdf[\"GNIS_Name\"].unique()):\n",
    "if checkpoint[\"river_index\"] >= len(rivers):\n",
    "    checkpoint[\"river_index\"] = 0\n",
    "    checkpoint[\"reach_index\"] = 0\n",
    "    json.dump(checkpoint, open(data_dir / \"reaches\" / \"checkpoint.json\", \"w\"))\n",
    "\n",
    "print(\"All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrothermal-history",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
