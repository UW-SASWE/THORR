{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = Path(\"../..\")\n",
    "\n",
    "# Create a directory to store in-situ data\n",
    "data_dir = proj_dir / \"insitu_data\"\n",
    "processed_data_dir = data_dir / \"processed\"\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "processed_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# load metadata\n",
    "stations_metadata_path = Path(proj_dir, \"insitu_data/metadata/stations.csv\")\n",
    "stations_attributes_path = Path(proj_dir, \"insitu_data/metadata/dictionaries/stations_attributes.csv\")\n",
    "\n",
    "stations_attributes = pd.read_csv(stations_attributes_path)\n",
    "\n",
    "conditions_data = pd.read_csv(Path(proj_dir, \"insitu_data/metadata/dictionaries/conditions_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "if not os.path.exists(stations_metadata_path):\n",
    "    stations_metadata = pd.DataFrame(columns=stations_attributes['Attribute_name'])\n",
    "    stations_metadata.to_csv(stations_metadata_path, index=False)\n",
    "\n",
    "stations_metadata = pd.read_csv(stations_metadata_path)\n",
    "\n",
    "usbr_stations_metadata = json.load(Path('stations.json').open(\"r\"))\n",
    "\n",
    "pcode_keys = usbr_stations_metadata[\"pcode_keys\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to format the url\n",
    "def format_url(station_name: str, pcodes: list, start: datetime, end: datetime):\n",
    "    \"\"\"Formats the url for the USBR PN data query.\n",
    "    Args:\n",
    "        station_name (str): The station name.\n",
    "        pcodes (list): The list of pcodes.\n",
    "        start (datetime): The start date.\n",
    "        end (datetime): The end date.\n",
    "    Returns:\n",
    "        url (str): The formatted url.\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        f\"https://www.usbr.gov/pn-bin/daily.pl?station={station_name.lower()}&format=csv&year={start.year}&month={start.month}&day={start.day}&year={end.year}&month={end.month}&day={end.day}\"\n",
    "        + \"\".join([\"&pcode=\" + pcode.strip(\" \").lower() for pcode in pcodes])\n",
    "    )\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to download the data for a station\n",
    "def download_data(station_name: str, pcodes: list, start: datetime, end: datetime, path: str):\n",
    "    \"\"\"Downloads the data for a station.\n",
    "    Args:\n",
    "        station_name (str): The station name.\n",
    "        pcodes (list): The list of pcodes.\n",
    "        start (datetime): The start date.\n",
    "        end (datetime): The end date.\n",
    "        path (str): The path to save the data.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # format the url\n",
    "    url = format_url(station_name, pcodes, start, end)\n",
    "\n",
    "    # download the data\n",
    "    # r = requests.get(url)\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.ConnectionError as e:\n",
    "        # sleep and try again\n",
    "        time.sleep(np.random.randint(20, 60))\n",
    "        r = requests.get(url)\n",
    "    # except requests.Timeout as e:\n",
    "    #     # stop the loop\n",
    "    #     break\n",
    "\n",
    "    # write the data to a csv file\n",
    "    with open(os.path.join(path, 'raw/usbr', station_name + \".csv\"), \"w\") as f:\n",
    "        f.write(r.text)\n",
    "\n",
    "    # read the csv file\n",
    "    with open(os.path.join(path, 'raw/usbr', station_name + \".csv\"), \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "\n",
    "    # remove the header\n",
    "    data = data[1:]\n",
    "\n",
    "    # define the column names\n",
    "    column_names = [\"date\"] + pcodes\n",
    "\n",
    "    # write the data to a csv file\n",
    "    with open(os.path.join(path, 'raw/usbr', station_name + \".csv\"), \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(column_names)\n",
    "        writer.writerows(data)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from deg min sec to decimal degrees\n",
    "def dms2dd(degrees, minutes=0, seconds=0, direction=None):\n",
    "    dd = float(degrees) + float(minutes) / 60 + float(seconds) / (60 * 60)\n",
    "    if direction == \"S\" or direction == \"W\":\n",
    "        dd *= -1\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process the downloaded data\n",
    "def postprocess_data(\n",
    "    station_name: str,\n",
    "    path: str,\n",
    "    grand_id: str = None,\n",
    "    pcodes: list = None,\n",
    "    pcode_keys: dict = None,\n",
    "):\n",
    "    if not grand_id:\n",
    "        grand_id = station_name\n",
    "\n",
    "    # read in the data\n",
    "    # print(path, \"raw/usbr\", \"{}.csv\".format(station_name.upper()))\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(path, \"raw/usbr\", \"{}.csv\".format(station_name.upper()))\n",
    "    )\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df[\"date\"] = df[\"date\"]\n",
    "\n",
    "    # convert the data to the correct units\n",
    "    for pcode in pcodes:\n",
    "        if pcode in pcode_keys.keys():\n",
    "            try:\n",
    "                pcode_keys[pcode][\"constant\"] = pcode_keys[pcode][\"constant\"]\n",
    "            except:\n",
    "                pcode_keys[pcode][\"constant\"] = None\n",
    "            \n",
    "            if pcode_keys[pcode][\"constant\"]:\n",
    "                new_df[pcode_keys[pcode][\"column_name\"]] = (\n",
    "                    df[pcode] * np.prod(pcode_keys[pcode][\"conversion_factors\"])\n",
    "                    + pcode_keys[pcode][\"constant\"]\n",
    "                )\n",
    "            else:\n",
    "                new_df[pcode_keys[pcode][\"column_name\"]] = df[pcode] * np.prod(\n",
    "                    pcode_keys[pcode][\"conversion_factors\"]\n",
    "                )\n",
    "\n",
    "    # save the data\n",
    "    new_df.to_csv(\n",
    "        # os.path.join(path, \"processed\", \"USBR_{}.csv\".format(grand_id)), index=False\n",
    "        os.path.join(path, \"processed\", \"USBR_{}.csv\".format(station_name)), index=False\n",
    "    )\n",
    "\n",
    "    return new_df.columns.tolist()\n",
    "    # print(\"processed data for {}\".format(station_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the station names\n",
    "# station_names = [\"crpo\", 'prv', 'prvo', 'kee', 'cle', 'crao']\n",
    "station_names = pd.read_csv(\"pcodes.csv\", header=None)[0]\n",
    "# grand_ids = [None, 91, '91_forebay', 55, 58, None]\n",
    "\n",
    "# define the start and end dates\n",
    "start_date = datetime.strptime(\"1982-01-01\", \"%Y-%m-%d\")\n",
    "end_date = pd.Timestamp.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed data for AFCI\n",
      "processed data for AGA\n",
      "processed data for ALNO\n",
      "processed data for ALPY\n",
      "processed data for ALTO\n",
      "processed data for AMF\n",
      "processed data for AMFI\n",
      "processed data for ANCI\n",
      "processed data for AND\n",
      "processed data for ANDI\n",
      "processed data for ANTI\n",
      "processed data for ANTO\n",
      "processed data for ARK\n",
      "processed data for ARNO\n",
      "processed data for ASCI\n",
      "processed data for AUCI\n",
      "processed data for BASO\n",
      "processed data for BCAO\n",
      "processed data for BCMO\n",
      "processed data for BCSO\n",
      "processed data for BCTO\n",
      "processed data for BDDI\n",
      "processed data for BENO\n",
      "processed data for BEU\n",
      "processed data for BEUO\n",
      "processed data for BFCI\n",
      "processed data for BFKY\n",
      "processed data for BFTI\n",
      "processed data for BIGI\n",
      "processed data for BILI\n",
      "processed data for BIRO\n",
      "processed data for BJBO\n",
      "processed data for BKPI\n",
      "processed data for BMCI\n",
      "processed data for BOOI\n",
      "processed data for BPPI\n",
      "processed data for BRFI\n",
      "processed data for BSEI\n",
      "processed data for BTSI\n",
      "processed data for BUL\n",
      "processed data for BUM\n",
      "processed data for BURI\n",
      "processed data for CACO\n",
      "processed data for CBCI\n",
      "processed data for CCL\n",
      "processed data for CCPI\n",
      "processed data for CCR\n",
      "processed data for CECI\n",
      "processed data for CENO\n",
      "processed data for CFCI\n",
      "processed data for CFMM\n",
      "processed data for CHEI\n",
      "processed data for CIBW\n",
      "processed data for CLE\n",
      "processed data for CLS\n",
      "processed data for CMO\n",
      "processed data for CRA\n",
      "processed data for CRAO\n",
      "processed data for CRCI\n",
      "processed data for CRCO\n",
      "processed data for CRE\n",
      "processed data for CREO\n",
      "processed data for CRPO\n",
      "processed data for CRSO\n",
      "processed data for CSAO\n",
      "processed data for CSC\n",
      "processed data for CSCI\n",
      "processed data for CSRO\n",
      "processed data for CVPI\n",
      "processed data for CXCI\n",
      "processed data for CXMI\n",
      "processed data for DCMO\n",
      "processed data for DEBO\n",
      "processed data for DED\n",
      "processed data for DEDI\n",
      "processed data for DICO\n",
      "processed data for DLEO\n",
      "processed data for DNCI\n",
      "processed data for DRYI\n",
      "processed data for EASW\n",
      "processed data for EBCO\n",
      "processed data for EGCI\n",
      "processed data for EGSO\n",
      "processed data for ELCI\n",
      "processed data for EMI\n",
      "processed data for EMM\n",
      "processed data for ENTI\n",
      "processed data for EPTO\n",
      "processed data for ERCI\n",
      "processed data for ETSI\n",
      "processed data for FALI\n",
      "processed data for FARI\n",
      "processed data for FCEO\n",
      "processed data for FCFM\n",
      "processed data for FCSO\n",
      "processed data for FFCI\n",
      "processed data for FHPI\n",
      "processed data for FIS\n",
      "processed data for FLGY\n",
      "processed data for FOR\n",
      "processed data for FRCI\n",
      "processed data for FRMO\n",
      "processed data for FSHO\n",
      "processed data for FURO\n",
      "processed data for GCGW\n",
      "processed data for GILO\n",
      "processed data for GLI\n",
      "processed data for GREY\n",
      "processed data for GRS\n",
      "processed data for GSPO\n",
      "processed data for GSTO\n",
      "processed data for GWCI\n",
      "processed data for HARI\n",
      "processed data for HAY\n",
      "processed data for HEII\n",
      "processed data for HEN\n",
      "processed data for HENI\n",
      "processed data for HFAI\n",
      "processed data for HGH\n",
      "processed data for HGHM\n",
      "processed data for HPCO\n",
      "processed data for HPD\n",
      "processed data for HRSI\n",
      "processed data for HYA\n",
      "processed data for IDCI\n",
      "processed data for INCI\n",
      "processed data for ISCI\n",
      "processed data for ISL\n",
      "processed data for ISLI\n",
      "processed data for JCK\n",
      "processed data for JKSY\n",
      "processed data for JRPI\n",
      "processed data for KAC\n",
      "processed data for KEE\n",
      "processed data for KIOW\n",
      "processed data for KTCW\n",
      "processed data for LABI\n",
      "processed data for LACI\n",
      "processed data for LAPO\n",
      "processed data for LBCO\n",
      "processed data for LBEO\n",
      "processed data for LEFO\n",
      "processed data for LNRW\n",
      "processed data for LORI\n",
      "processed data for LOW\n",
      "processed data for LPPI\n",
      "processed data for LUC\n",
      "processed data for LWOI\n",
      "processed data for MABO\n",
      "processed data for MADO\n",
      "processed data for MALO\n",
      "processed data for MAN\n",
      "processed data for MAXO\n",
      "processed data for MCK\n",
      "processed data for MCKO\n",
      "processed data for MFDO\n",
      "processed data for MIII\n",
      "processed data for MIL\n",
      "processed data for MILI\n",
      "processed data for MIN\n",
      "processed data for MINI\n",
      "processed data for MLCI\n",
      "processed data for MLCO\n",
      "processed data for MORI\n",
      "processed data for MPCI\n",
      "processed data for MRYI\n",
      "processed data for MXCI\n",
      "processed data for MYKO\n",
      "processed data for NACW\n",
      "processed data for NCAO\n",
      "processed data for NFLO\n",
      "processed data for NLCI\n",
      "processed data for NMCO\n",
      "processed data for NPAO\n",
      "processed data for NPDO\n",
      "processed data for OCH\n",
      "processed data for OCHO\n",
      "processed data for OCRO\n",
      "processed data for OSCI\n",
      "processed data for OWY\n",
      "processed data for PAL\n",
      "processed data for PALI\n",
      "processed data for PARI\n",
      "processed data for PARW\n",
      "processed data for PAY\n",
      "processed data for PAYI\n",
      "processed data for PCKY\n",
      "processed data for PDTO\n",
      "processed data for PECI\n",
      "processed data for PHL\n",
      "processed data for PLCI\n",
      "processed data for PLEI\n",
      "processed data for POCI\n",
      "processed data for PRHO\n",
      "processed data for PRKI\n",
      "processed data for PRLI\n",
      "processed data for PRPI\n",
      "processed data for PRV\n",
      "processed data for PRVO\n",
      "processed data for PWDO\n",
      "processed data for RBDW\n",
      "processed data for RDWI\n",
      "processed data for RECI\n",
      "processed data for REXI\n",
      "processed data for RGCI\n",
      "processed data for RICI\n",
      "processed data for RIM\n",
      "processed data for RIR\n",
      "processed data for RJPI\n",
      "processed data for ROMO\n",
      "processed data for ROZW\n",
      "processed data for RRPI\n",
      "processed data for RSA\n",
      "processed data for RSCW\n",
      "processed data for RSDI\n",
      "processed data for RXRI\n",
      "processed data for SALY\n",
      "processed data for SBCO\n",
      "processed data for SBEO\n",
      "processed data for SCLO\n",
      "processed data for SCO\n",
      "processed data for SCOO\n",
      "processed data for SDCO\n",
      "processed data for SFLO\n",
      "processed data for SHYI\n",
      "processed data for SLBO\n",
      "processed data for SNAI\n",
      "processed data for SNCW\n",
      "processed data for SNDI\n",
      "processed data for SOL\n",
      "processed data for SPBO\n",
      "processed data for SPPI\n",
      "processed data for SQSO\n",
      "processed data for SUCI\n",
      "processed data for SWCO\n",
      "processed data for TCNI\n",
      "processed data for TCSI\n",
      "processed data for TEAI\n",
      "processed data for TEAW\n",
      "processed data for TFCO\n",
      "processed data for TGCI\n",
      "processed data for THF\n",
      "processed data for TICI\n",
      "processed data for TICW\n",
      "processed data for TIEW\n",
      "processed data for TITI\n",
      "processed data for TLCI\n",
      "processed data for TNAW\n",
      "processed data for TNCM\n",
      "processed data for TRCI\n",
      "processed data for TRGO\n",
      "processed data for TUMO\n",
      "processed data for UMAO\n",
      "processed data for UMDO\n",
      "processed data for UMMO\n",
      "processed data for UMTW\n",
      "processed data for UMUO\n",
      "processed data for UNY\n",
      "processed data for VALO\n",
      "processed data for VLCI\n",
      "processed data for VSPI\n",
      "processed data for WACI\n",
      "processed data for WAFI\n",
      "processed data for WAH\n",
      "processed data for WAR\n",
      "processed data for WARO\n",
      "processed data for WAS\n",
      "processed data for WBCO\n",
      "processed data for WBFI\n",
      "processed data for WCRO\n",
      "processed data for WEIO\n",
      "processed data for WEPO\n",
      "processed data for WESO\n",
      "processed data for WEUO\n",
      "processed data for WFCI\n",
      "processed data for WGCM\n",
      "processed data for WIC\n",
      "processed data for WICO\n",
      "processed data for WIFI\n",
      "processed data for WILI\n",
      "processed data for WLD\n",
      "processed data for WLDN\n",
      "processed data for WOD\n",
      "processed data for WODI\n",
      "processed data for WOPW\n",
      "processed data for WTXI\n",
      "processed data for WVCI\n",
      "processed data for YOKO\n",
      "processed data for YRPW\n",
      "processed data for YRWW\n"
     ]
    }
   ],
   "source": [
    "# read the stations json file\n",
    "with open(\"stations.json\", \"r\") as f:\n",
    "    stations_dict = json.load(f)\n",
    "\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path)\n",
    "os.makedirs(os.path.join(data_dir, \"raw/usbr\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, \"processed\"), exist_ok=True)\n",
    "\n",
    "# download the data for each station\n",
    "# for station_name, id in zip(station_names, grand_ids):\n",
    "for station_name in station_names:\n",
    "    # if pcodes exist for the station\n",
    "    if \"pcodes\" in stations_dict[station_name.upper()]:\n",
    "        # define the pcodes and pcode keys\n",
    "        pcodes = stations_dict[station_name.upper()][\"pcodes\"]\n",
    "        pcode_keys = stations_dict[\"pcode_keys\"]\n",
    "\n",
    "        # download the data\n",
    "        download_data(station_name.upper(), pcodes, start_date, end_date, data_dir)\n",
    "        # postprocess the data\n",
    "        station_conditions = postprocess_data(\n",
    "            station_name.upper(), data_dir, pcodes=pcodes, pcode_keys=pcode_keys\n",
    "        )\n",
    "\n",
    "        # print(conditions_data)\n",
    "\n",
    "        # update the metadata\n",
    "        station_ID = f\"USBR_{station_name}\"\n",
    "\n",
    "        # if station_ID not in stations_metadata[\"station_ID\"].values:\n",
    "        #     stations_metadata = pd.concat(\n",
    "        #         [\n",
    "        #             stations_metadata,\n",
    "        #             pd.DataFrame(\n",
    "        #                 {\n",
    "        #                     \"station_ID\": [station_ID],\n",
    "        #                     \"id_at_source\": [station_name.upper()],\n",
    "        #                     \"available_data\": [\"{}\"],\n",
    "        #                     \"source_URL\": [\n",
    "        #                         f\"https://www.usbr.gov/pn-bin/inventory.pl?site={station_name.upper()}&ui=true&interval=daily\"\n",
    "        #                     ],\n",
    "        #                     \"description\": [\n",
    "        #                         usbr_stations_metadata[station_name][\"description\"]\n",
    "        #                     ],\n",
    "        #                     \"latitude\": [\n",
    "        #                         dms2dd(\n",
    "        #                             *usbr_stations_metadata[station_name][\"latitude\"]\n",
    "        #                             .strip(\"-\")\n",
    "        #                             .split(\"-\"),\n",
    "        #                             direction=\"N\",\n",
    "        #                         )\n",
    "        #                     ],\n",
    "        #                     \"longitude\": [\n",
    "        #                         dms2dd(\n",
    "        #                             direction=\"W\",\n",
    "        #                             *usbr_stations_metadata[station_name][\"longitude\"]\n",
    "        #                             .strip(\"-\")\n",
    "        #                             .split(\"-\"),\n",
    "        #                         )\n",
    "        #                     ],\n",
    "        #                     \"site_params\": [\"{}\"],\n",
    "        #                 }\n",
    "        #             ),\n",
    "        #         ],\n",
    "        #         ignore_index=True,\n",
    "        #     )\n",
    "\n",
    "        # # update the available data\n",
    "        # availble_data = stations_metadata.loc[\n",
    "        #     stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "        # ].values[0]\n",
    "        # availble_data = json.loads(availble_data)\n",
    "\n",
    "        # add the parameters to the available data\n",
    "        # print(parameters[1:])\n",
    "        \n",
    "        for param in station_conditions:\n",
    "\n",
    "            if station_ID not in stations_metadata[\"station_ID\"].values:\n",
    "                new_stations_metadata = pd.DataFrame(\n",
    "                    {\n",
    "                        \"station_ID\": [station_ID],\n",
    "                        \"id_at_source\": [station_name.upper()],\n",
    "                        \"available_data\": [\"{}\"],\n",
    "                        \"source_URL\": ['{\"url\" : []}'],\n",
    "                        \"description\": [\n",
    "                            usbr_stations_metadata[station_name][\"description\"]\n",
    "                        ],\n",
    "                        \"latitude\": [\n",
    "                            dms2dd(\n",
    "                                *usbr_stations_metadata[station_name][\"latitude\"]\n",
    "                                .strip(\"-\")\n",
    "                                .split(\"-\"),\n",
    "                                direction=\"N\",\n",
    "                            )\n",
    "                        ],\n",
    "                        \"longitude\": [\n",
    "                            dms2dd(\n",
    "                                direction=\"W\",\n",
    "                                *usbr_stations_metadata[station_name][\"longitude\"]\n",
    "                                .strip(\"-\")\n",
    "                                .split(\"-\"),\n",
    "                            )\n",
    "                        ],\n",
    "                        \"site_params\": [\"{}\"],\n",
    "                    }\n",
    "                )\n",
    "                availble_data = json.loads(\n",
    "                    new_stations_metadata[\"available_data\"].values[0]\n",
    "                )\n",
    "                stations_metadata = pd.concat(\n",
    "                    [\n",
    "                        stations_metadata,\n",
    "                        new_stations_metadata,\n",
    "                    ],\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                availble_data = stations_metadata.loc[\n",
    "                    stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "                ].values[0]\n",
    "                availble_data = json.loads(availble_data)\n",
    "\n",
    "            # update source url\n",
    "            source_url = json.loads(\n",
    "                stations_metadata.loc[\n",
    "                    stations_metadata[\"station_ID\"] == station_ID, \"source_URL\"\n",
    "                ].values[0]\n",
    "            )\n",
    "\n",
    "            if (\n",
    "                f\"https://www.usbr.gov/pn-bin/inventory.pl?site={station_name.upper()}&ui=true&interval=daily\"\n",
    "                not in source_url[\"url\"]\n",
    "            ):\n",
    "                source_url[\"url\"].append(\n",
    "                    f\"https://www.usbr.gov/pn-bin/inventory.pl?site={station_name.upper()}&ui=true&interval=daily\"\n",
    "                )\n",
    "                stations_metadata.loc[\n",
    "                    stations_metadata[\"station_ID\"] == station_ID, \"source_URL\"\n",
    "                ] = json.dumps(source_url)\n",
    "\n",
    "            # check if there is \"conditions\"  in the available data\n",
    "            if \"conditions\" not in availble_data.values():\n",
    "                availble_data[\"conditions\"] = []\n",
    "\n",
    "            # # update the available data\n",
    "            # availble_data = stations_metadata.loc[\n",
    "            #     stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "            # ].values[0]\n",
    "            # availble_data = json.loads(availble_data)\n",
    "\n",
    "            # print((param not in availble_data[\"conditions\"]) and (param in conditions_data['Attribute_name'].to_list()))\n",
    "            if (param not in availble_data[\"conditions\"]) and (\n",
    "                param in conditions_data[\"Attribute_name\"].to_list()\n",
    "            ):\n",
    "\n",
    "                availble_data[\"conditions\"].append(param)\n",
    "\n",
    "                # if station_ID not in stations_metadata[\"station_ID\"].values:\n",
    "                #     new_stations_metadata[\"available_data\"] = json.dumps(availble_data)\n",
    "                #     stations_metadata = pd.concat(\n",
    "                #         [\n",
    "                #             stations_metadata,\n",
    "                #             new_stations_metadata,\n",
    "                #         ],\n",
    "                #         ignore_index=True,\n",
    "                #     )\n",
    "                # else:\n",
    "                #     # update the metadata\n",
    "                #     stations_metadata.loc[\n",
    "                #         stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "                #     ] = json.dumps(availble_data)\n",
    "\n",
    "                # update the metadata\n",
    "                stations_metadata.loc[\n",
    "                    stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "                ] = json.dumps(availble_data)\n",
    "\n",
    "                # # update the metadata\n",
    "                # stations_metadata.loc[\n",
    "                #     stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "                # ] = json.dumps(availble_data)\n",
    "\n",
    "                # save the metadata\n",
    "                stations_metadata.to_csv(stations_metadata_path, index=False)\n",
    "\n",
    "    print(\"processed data for {}\".format(station_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add last updated date and last updated by\n",
    "metadata_status = {\n",
    "    \"last_updated\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"update_message\": \"Updated the metadata to for USGS stations\",\n",
    "    \"last_updated_by\": \"George Darkwah\",\n",
    "    \"last_updated_by_email\": \"gdarkwah@uw.edu\",\n",
    "}\n",
    "\n",
    "# save metadata\n",
    "with open(Path(data_dir, \"metadata/metadata_status.csv\"), \"w\") as f:\n",
    "    json.dump(metadata_status, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrothermal-history",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
