{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from thorr.utils import read_config, Logger, validate_start_end_dates\n",
    "from thorr.database import Connect as db_connect\n",
    "\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import root_mean_squared_error, mean_squared_error, r2_score\n",
    "\n",
    "# from sklearn.model_selection import KFold, ShuffleSplit, RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = Path(\"/Users/gdarkwah/Desktop/CRITFC_20251107/.env/v1.0.0_config.ini\")\n",
    "insitu_data_dir = Path(\"insitu_data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = read_config(config_file)\n",
    "# project_dir = Path(config_dict[\"project\"][\"project_dir\"])\n",
    "project_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = db_connect(config_file,)\n",
    "schema = db.schema\n",
    "\n",
    "connection = db.connection\n",
    "cursor = connection.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the basin GIS data\n",
    "basin_query = f\"\"\"\n",
    "SELECT \n",
    "    \"RegionID\" as region_id,\n",
    "    \"Name\",\n",
    "    ST_AsBinary(\"geometry\") AS geometry,\n",
    "    ST_SRID(\"geometry\") AS srid\n",
    "FROM {schema}.\"Regions\";\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(basin_query)\n",
    "basin_gdf = gpd.GeoDataFrame(\n",
    "    cursor.fetchall(),\n",
    "    columns=[col[0] for col in cursor.description],\n",
    ")\n",
    "\n",
    "basin_gdf[\"geometry\"] = gpd.GeoSeries.from_wkb(basin_gdf[\"geometry\"])\n",
    "basin_gdf = gpd.GeoDataFrame(basin_gdf, geometry=\"geometry\")\n",
    "basin_gdf = basin_gdf.set_crs(epsg=basin_gdf[\"srid\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve GIS data for reaches\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    \"ReachID\" AS reach_id,\n",
    "    \"Name\" AS reach_name,\n",
    "    \"RiverID\" AS river_id,\n",
    "    \"RKm\",\n",
    "    ST_AsBinary(\"buffered_geometry\") AS geometry,\n",
    "    ST_SRID(\"buffered_geometry\") AS srid\n",
    "FROM\n",
    "    {schema}.\"Reaches\"\n",
    "ORDER By\n",
    "    \"ReachID\";\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "reaches_gdf = pd.DataFrame(\n",
    "    cursor.fetchall(),\n",
    "    columns=[col[0] for col in cursor.description],\n",
    ")\n",
    "reaches_gdf[\"geometry\"] = gpd.GeoSeries.from_wkb(reaches_gdf[\"geometry\"])\n",
    "reaches_gdf = gpd.GeoDataFrame(reaches_gdf, geometry=\"geometry\")\n",
    "reaches_gdf = reaches_gdf.set_crs(epsg=reaches_gdf[\"srid\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stations_metadata_path = Path(project_dir, \"insitu_data/metadata/stations.csv\")\n",
    "\n",
    "stations_df = pd.read_csv(stations_metadata_path)\n",
    "stations_gdf = gpd.GeoDataFrame(\n",
    "    stations_df,\n",
    "    geometry=gpd.points_from_xy(stations_df.longitude, stations_df.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "\n",
    "stations_fn = \"stations_gis/stations.gpkg\"\n",
    "stations = gpd.read_file(stations_fn, layer='Stations')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot temperature locations\n",
    "fig, ax = plt.subplots()\n",
    "basin_gdf.plot(ax=ax, color='lightgray')\n",
    "reaches_gdf.plot(ax=ax, color='blue')\n",
    "stations.plot(ax=ax, color='black', markersize=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_in_reaches = stations[stations.within(reaches_gdf.unary_union)].copy()\n",
    "reaches_gdf.drop(columns=[col for col in reaches_gdf.columns if col in ['index_right', 'index_left']], inplace=True)\n",
    "stations_in_reaches.drop(columns=[col for col in stations_in_reaches.columns if col in ['index_right', 'index_left']], inplace=True)\n",
    "\n",
    "stations_in_reaches = stations_in_reaches.sjoin(reaches_gdf, how='left', predicate='within')\n",
    "# stations_in_reaches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter temperature locations within the buffered\n",
    "fig, ax = plt.subplots()\n",
    "basin_gdf.plot(ax=ax, color='lightgray')\n",
    "reaches_gdf.plot(ax=ax, color='blue')\n",
    "stations_in_reaches.plot(ax=ax, color='red', markersize=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Retrieve the reach data necessary for training the model\n",
    "# query1 = f\"\"\"\n",
    "# SELECT\n",
    "#     \"ReachID\",\n",
    "#     \"RKm\",\n",
    "#     \"Date\",\n",
    "#     \"LandTempC\",\n",
    "#     \"WaterTempC\",\n",
    "#     \"NDVI\",\n",
    "#     \"Mission\",\n",
    "#     \"WidthMin\",\n",
    "#     \"WidthMean\",\n",
    "#     \"WidthMax\",\n",
    "#     \"Name\",\n",
    "#     \"ClimateClass\",\n",
    "#     \"EstTempC\"\n",
    "# FROM\n",
    "#     {schema}.\"ReachData\"\n",
    "#     LEFT JOIN {schema}.\"Reaches\" USING (\"ReachID\")\n",
    "# WHERE\n",
    "#     \"LandTempC\" IS NOT NULL\n",
    "#     AND \"NDVI\" IS NOT NULL;\n",
    "# \"\"\"\n",
    "\n",
    "# with connection.cursor() as cursor:\n",
    "#     cursor.execute(query1)\n",
    "#     lsat_data = pd.DataFrame(\n",
    "#         cursor.fetchall(), columns=[desc[0] for desc in cursor.description]\n",
    "#     )\n",
    "#     lsat_data[\"Date\"] = pd.to_datetime(lsat_data[\"Date\"])\n",
    "\n",
    "lsat_data = pd.read_csv(\n",
    "    project_dir / \"thorr_data/lsat_data.csv\"\n",
    ")\n",
    "lsat_data[\"Date\"] = pd.to_datetime(lsat_data[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DOY column\n",
    "lsat_data[\"DOY\"] = lsat_data[\"Date\"].dt.dayofyear\n",
    "# fill na values of the mean width values with 15\n",
    "lsat_data[[\"WidthMean\"]] = lsat_data[[\"WidthMean\"]].fillna(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features\n",
    "features = [\n",
    "    \"NDVI\",\n",
    "    \"LandTempC\",\n",
    "    \"ClimateClass\",\n",
    "    \"DOY\",\n",
    "    \"WidthMean\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_insitu = pd.DataFrame()\n",
    "\n",
    "for station_loc in stations_in_reaches.itertuples():\n",
    "    insitu_fn = insitu_data_dir / f\"{station_loc.station_ID}.csv\"\n",
    "    if insitu_fn.exists():\n",
    "        insitu_data = pd.read_csv(insitu_fn, parse_dates=['date'])\n",
    "        # print(insitu_data.columns)\n",
    "        # drop utc in date\n",
    "        insitu_data['date'] = insitu_data['date'].dt.tz_localize(None)\n",
    "\n",
    "        # keep date and average temperature\n",
    "        # insitu_data = insitu_data[['date', 'avg_temp(C)', ]]\n",
    "        # replace -999999 with nan\n",
    "        # insitu_data.replace(-999999, np.nan, inplace=True)\n",
    "        insitu_data.dropna(inplace=True)\n",
    "        insitu_data['station_ID'] = station_loc.station_ID\n",
    "        insitu_data['RKm'] = station_loc.RKm\n",
    "        insitu_data['Name'] = station_loc.reach_name\n",
    "        combined_insitu = pd.concat([combined_insitu, insitu_data])\n",
    "        # break\n",
    "\n",
    "# combined_insitu = pd.concat(combined_insitu)\n",
    "\n",
    "combined_insitu.rename(columns={'date': 'Date'}, inplace=True)\n",
    "# combined_insitu = combined_insitu.merge(lsat_data, on='Date', how='left')\n",
    "# Data/insitu/conditions/processed\n",
    "# combined_insitu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insitu_lsat = pd.merge(combined_insitu, lsat_data, on=['Date', 'Name', 'RKm'], how='inner')\n",
    "all_data = lsat_data.merge(combined_insitu, on=['Date', 'Name', 'RKm'], how='outer')\n",
    "insitu_lsat_clean = insitu_lsat[(insitu_lsat['avg_temp(C)'] >=0) & (insitu_lsat['WaterTempC'] >=-40) & (insitu_lsat['WaterTempC'] <= 80) & (insitu_lsat['avg_temp(C)'] <= 40)].copy()\n",
    "insitu_lsat_clean[['WidthMin', 'WidthMean', 'WidthMax']] = insitu_lsat_clean[['WidthMin', 'WidthMean', 'WidthMax']].fillna(15)\n",
    "all_data[['WidthMin', 'WidthMean', 'WidthMax']] = all_data[['WidthMin', 'WidthMean', 'WidthMax']].fillna(15)\n",
    "\n",
    "\n",
    "insitu_lsat_clean[\"DOY\"] = insitu_lsat_clean[\"Date\"].dt.dayofyear\n",
    "all_data[\"DOY\"] = all_data[\"Date\"].dt.dayofyear\n",
    "\n",
    "# insitu_lsat_clean.to_csv('ml_inputs_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "insitu_lsat_clean = all_data.copy()\n",
    "insitu_lsat_clean.to_csv('ml_inputs_data.csv', index=False)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scalers\n",
    "doy_scaler = MinMaxScaler(feature_range=(0, 1)).fit(pd.DataFrame({'DOY': range(1, 365)}))\n",
    "watertemp_scaler = StandardScaler().fit(insitu_lsat_clean[['WaterTempC']])\n",
    "landtemp_scaler = StandardScaler().fit(insitu_lsat_clean[['LandTempC']])\n",
    "# watertemp_scaler = MinMaxScaler(feature_range=(0, 1)).fit(df3[['WaterTemp']])\n",
    "# landtemp_scaler = MinMaxScaler(feature_range=(0, 1)).fit(df3[['LandTemp']])\n",
    "width_max_scaler = MinMaxScaler(feature_range=(0, 1)).fit(insitu_lsat_clean[['WidthMax']])\n",
    "width_mean_scaler = MinMaxScaler(feature_range=(0, 1)).fit(insitu_lsat_clean[['WidthMean']])\n",
    "width_min_scaler = MinMaxScaler(feature_range=(0, 1)).fit(insitu_lsat_clean[['WidthMin']])\n",
    "NDVI_scaler = StandardScaler().fit(insitu_lsat_clean[['NDVI']])\n",
    "# NDVI_scaler = MinMaxScaler(feature_range=(-1, 1)).fit(df3[['NDVI']])\n",
    "climate_scaler = MinMaxScaler(feature_range=(0, 1)).fit(pd.DataFrame({'ClimateClass': range(1, 30+1)}))\n",
    "avg_temp_scaler = StandardScaler().fit(insitu_lsat_clean[['avg_temp(C)']])\n",
    "\n",
    "# define a directory to save the scalers\n",
    "scalers_dir = Path('scalers')\n",
    "scalers_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save the scalers\n",
    "dump(doy_scaler, 'scalers/doy_scaler.joblib')\n",
    "dump(watertemp_scaler, 'scalers/watertemp_scaler.joblib')\n",
    "dump(landtemp_scaler, 'scalers/landtemp_scaler.joblib')\n",
    "dump(width_max_scaler, 'scalers/width_max_scaler.joblib')\n",
    "dump(width_mean_scaler, 'scalers/width_mean_scaler.joblib')\n",
    "dump(width_min_scaler, 'scalers/width_min_scaler.joblib')\n",
    "dump(NDVI_scaler, 'scalers/NDVI_scaler.joblib')\n",
    "dump(climate_scaler, 'scalers/climate_scaler.joblib')\n",
    "dump(avg_temp_scaler, 'scalers/avg_temp_scaler.joblib')\n",
    "\n",
    "# # load the scalers\n",
    "# doy_scaler = load('scalers/doy_scaler.joblib')\n",
    "# watertemp_scaler = load('scalers/watertemp_scaler.joblib')\n",
    "# landtemp_scaler = load('scalers/landtemp_scaler.joblib')\n",
    "# width_max_scaler = load('scalers/width_max_scaler.joblib')\n",
    "# width_mean_scaler = load('scalers/width_mean_scaler.joblib')\n",
    "# width_min_scaler = load('scalers/width_min_scaler.joblib')\n",
    "# NDVI_scaler = load('scalers/NDVI_scaler.joblib')\n",
    "# climate_scaler = load('scalers/climate_scaler.joblib')\n",
    "# avg_temp_scaler = load('scalers/avg_temp_scaler.joblib')\n",
    "\n",
    "\n",
    "# # scale the data\n",
    "# insitu_lsat_clean['DOY_scaled'] = doy_scaler.transform(insitu_lsat_clean[['DOY']])\n",
    "# insitu_lsat_clean['WaterTempC_scaled'] = watertemp_scaler.transform(insitu_lsat_clean[['WaterTempC']])\n",
    "# insitu_lsat_clean['LandTempC_scaled'] = landtemp_scaler.transform(insitu_lsat_clean[['LandTempC']])\n",
    "# insitu_lsat_clean['WidthMax_scaled'] = width_max_scaler.transform(insitu_lsat_clean[['WidthMax']])\n",
    "# insitu_lsat_clean['WidthMean_scaled'] = width_mean_scaler.transform(insitu_lsat_clean[['WidthMean']])\n",
    "# insitu_lsat_clean['WidthMin_scaled'] = width_min_scaler.transform(insitu_lsat_clean[['WidthMin']])\n",
    "# insitu_lsat_clean['NDVI_scaled'] = NDVI_scaler.transform(insitu_lsat_clean[['NDVI']])\n",
    "# insitu_lsat_clean['ClimateClass_scaled'] = climate_scaler.transform(insitu_lsat_clean[['ClimateClass']])\n",
    "# insitu_lsat_clean['avg_temp(C)_scaled'] = avg_temp_scaler.transform(insitu_lsat_clean[['avg_temp(C)']])\n",
    "\n",
    "# # insitu_lsat_clean.to_csv('ml_input_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thorr_test_v1.0.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
